<!DOCTYPE html>
<html lang="en"><head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Google Font -->
    <link rel="stylesheet" href="https://use.typekit.net/vhi7kwp.css">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="/bootstrap/css/bootstrap.css">
    <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/headerfooterstyle.css">
    <link rel="stylesheet" href="/css/indexstyle.css">
    <link rel="stylesheet" href="/css/aboutstyle.css">
    <link href="/css/code_highlight.css" rel="stylesheet">
    <!-- <link rel="stylesheet" href="assets/main.scss"> -->

    <title>Fractal</title>
    <link rel="icon" type="image/png" href="/imgs/favicon2.png">

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
        },
        "HTML-CSS": { availableFonts: ["TeX"] },
        }
        );
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head><body><!-- NAVBAR -->
<nav id="mainNavbar" class="navbar navbar-dark navbar-expand-md py-0 fixed-top">
    <a class="navbar-brand" href="/index.html">
        <img src="/imgs/logo.png" id="logo-img" style="padding: 5px;">
    </a>
    <button class="navbar-toggler navbar-toggler-right" data-toggle="collapse" data-target="#navLinks" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navLinks">
        <ul class="nav navbar-nav ml-auto">
            <li class="nav-item">
                <a href="/content.html" class="nav-link">Lessons</a>
            </li>
            <li class="nav-item">
                <a href="/volunteer.html" class="nav-link">For Mentors</a>
            </li>
            <li class="nav-item">
                <a href="/about.html" class="nav-link">About Us</a>
            </li>
            <!-- <li class="nav-item">
                <a href="sponsors.html" class="nav-link">Sponsor Us</a>
            </li>
            <li class="nav-item">
                <a href="current_members.html" class="nav-link">Alumni</a>
            </li> -->
            <li class="nav-item">
                <a href="https://forms.gle/KLYDQmpY7ge3vZ3i6" target="_blank">
                    <button class="btn btn-outline-success my-2 my-sm-0 nav-apply-btn" type="submit">Apply</button>
                </a>
            </li>
        </ul>
    </div>
</nav><article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Q-Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-02-05T03:06:43-05:00" itemprop="datePublished">Feb 5, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="learning-based-methods">Learning-Based Methods</h3>

<p>Policy and Value Iteration gave us a solid way to find the optimal policy when we have perfect information about the environment (i.e. we know the distributions of state transitions and rewards), but when this information is not know, we have to get clever with how we determine good policies. One way is to learn by trial and error - taking actions in the environment and observing what states we transition to under different actions and what rewards we obtain for doing so. Doing this gives us data in the form $(s, a, r, s’)$. If we take action $a$ in state $s$ we receive reward $r$ and transition to state $s’$. From this data we can try to approximate the unknown distributions.</p>

<p>Another issue we face is large state spaces. Policy and Value iteration worked fine for Gridworld (small state space), but when the total number of states becomes large, these algorithms become intractable - they contain a $|\mathbf{S}|^{3}$ and a $|\mathbf{S}|^{2}$ term respectively in their time complexities! Our solution to this issue is to learn a lower-dimensional representation of the state using neural networks. This is known as deep reinforcement learning, and the type we will be exploring in this guide is called deep Q-Learning.</p>

<h3 id="deep-q-learning">Deep Q-Learning</h3>

<p>Essentially what we are trying to do is approximate $Q^{\ast}(s, a)$ using a neural network. If we can get a good approximation of $Q^{\ast}$, we can extract a good policy. This neural network will be parameterized by a generic term $\theta$, will take as input the state $s$ and output value for each possible action, which we can perform a max operation over to get the best action to take.</p>

<p>In order to learn such a function, we need to define a loss function so that our network knows what it’s optimizing for. Recall that the optimal Q function satisfies</p>

\[Q^{\ast}(s,a) = \mathbb{E}_{s' \sim p(s'|s, a)}\bigg[r(s, a) + \gamma \max_{a'}Q^{\ast}(s', a')\bigg]\]

<p>Assume we have a bunch of data in the form ${(s, a, r, s’)}_{i=1}^{N}$. Then for one of the data points, we can measure how close our Q network approximates the optimal Q function by the following equation.</p>

\[\text{MSE Loss} = (Q_{t}(s, a) - [r + \max_{a} Q_{t-1}(s', a)])^{2}\]

<p>Where $Q_{t}$ and $Q_{t-1}$ represent the network output before and after a single weight update. Notice how the first term in the square is our network’s current output and the second term is the target Q-value that we want, but based on the old network weights. The training pipeline looks something like below. First we college a batch of data (i.e. the agent takes actions in the environment) of size $B$, then we feed that data into the network, compute the loss, and update our network weights. Below, $D$ is the dimensionality of the state representation (e.g. the number of pixels in an image).</p>

<center>
  <div class="col-lg-10 col-md-10 col-sm-12 col-xs-12">
    <img src="/assets/Deep_Q_Learning/q-network-training.png" />
  </div>
</center>

<hr />

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<!-- horizontal -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-8495937332177101" data-ad-slot="8539861386" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>

<hr />

<h3 id="epsilon-greedy-and-experience-replay">Epsilon Greedy and Experience Replay</h3>

<p>This framework gives us a good way to approximate the optimal Q function, but there still remains the question of how do we actually collect the data? What policy should we use for that? To better explain this problem let’s consider an example. Say we have some sub-optimal policy $\pi_{0}$ that we will use to collect experience $(s, a, r, s’)$ data in the environment. If we simply choose the best action for each state according to this sub-optimal policy, we may not discover that some actions that are not chosen by $\pi_{0}$ lead to good rewards. Essentially, we will be stuck in local minima. One way around this is to occasionally take random actions so that we have a chance of seeing new experiences and hopefully finding better actions to take. This is an exploration strategy known as epsilon-greedy. It says that for some time $t$ the action we choose should be made according to the following rule.</p>

\[a_{t} =
\begin{cases}
\arg\max_{a}Q_{t}(s, a) &amp; \text{with probability } 1 - \epsilon \\
\text{random action} &amp; \text{with probability } \epsilon
\end{cases}\]

<p>This will allow our agent to do some exploring to find good state-action combinations. Typically, it is good to do a lot of exploration when the network first starts training by using a high value for epsilon, reducing epsilon gradually as training progresses.</p>

<p>The next issue we run into is that consecutive data is highly correlated, which can lead to feedback loops or just really slow training. For example, if we are gathering data under a policy that tells the agent to move down, then data that represents this type of action will be overrepresented in the next iteration of training even though the better option might be to go right and we just haven’t explored that yet. To solve this, one solution is to maintain a buffer that stores data $(s, a, r, s’)$ that we continually update while the agent moves through the environment, removing old data as the buffer gets full. When it comes time to sample a batch of data for training, we randomly sample from this buffer rather that take a bunch of consecutive data like before. This approach is called experience replay.</p>

<p>Armed with the knowledge of these common problems and some solid ways to address them, we present the full Deep Q-Learning algorithm with Experience Replay.</p>

<center>
  <figure>
    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
      <img src="/assets/Deep_Q_Learning/DQN-algorithm.png" />
      <figcaption>Credit: Fei-Fei Li, Justin Johnson, Serena Yeung: CS231n</figcaption>
    </div>
  </figure>
</center>

<p>The function $\phi$ is just a preprocessing step before inputting the data into the neural network and can be ignored for our purposes. The curious reader can explore the full paper from DeepMind: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>.</p>

<p>We have seen a method for approximating Q function using neural networks by gathering experience data from within the environment and using it to train the network as well as some problems that arise from this approach. We have also seen reasonable ways to deal with these problems. Next, we will learn methods for estimating the optimal policy without going through the middle-man of estimating a Q function.</p>

<hr />

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<!-- horizontal -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-8495937332177101" data-ad-slot="8539861386" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>

<hr />


  </div><a class="u-url" href="/dl/2020/02/05/deep-q-learning.html" hidden></a>
</article>
<!-- FOOTER -->
<footer class="container-fluid footer text-center align-items-center justify-content-center">
    <section class="container-fluid px-0 text-center align-items-center justify-content-center pink-bg">
        <h3 class="mb-4">Stay Informed!</h3>
        <!-- <form action="emails.php">
            <input type="text" placeholder="Email Address" name="mail" required="">
            <input type="submit" value="Submit">
        </form> -->
        <a href="https://forms.gle/HQpw1VeFUPN8uki4A" target="_">
            <button class="btn btn-outline-success my-2 my-sm-0" type="submit">Join our mailing list!</button>
        </a>
    </section>
    <section class="container-fluid px-0 text-center align-items-center justify-content-center row">
        <!-- <div class="col-md-2 mx-0 align-items-center justify-content-center">
            <h5 class="font-weight-bold text-uppercase mt-5 mb-4">About Us</h5>
            <ul class="list-unstyled">
                <li>
                    <a href="about.html" class="lead">Our Mission</a>
                </li>
                <li>
                    <a href="committees.html" class="lead">Our Team</a>
                </li>
            </ul>
        </div> -->
        <!-- <div class="col-md-2 mx-0 align-items-center justify-content-center">
            <h5 class="font-weight-bold text-uppercase mt-5 mb-4">Impact</h5>
            <ul class="list-unstyled">
                <li>
                    <a href="about.html#about-press" class="lead">Press</a>
                </li>
                <li>
                    <a href="https://sites.google.com/umich.edu/cskickstart/home" class="lead" target="_">Sister Program</a>
                </li>
            </ul>
        </div> -->
        <div class="col-md-2 mx-0 align-items-center justify-content-center">
            <h5 class="font-weight-bold text-uppercase mt-5 mb-4">Our Program</h5>
            <ul class="list-unstyled">
                <li>
                    <a href="https://forms.gle/KLYDQmpY7ge3vZ3i6" class="lead">Apply</a>
                </li>
                <li>
                    <a href="/about.html" class="lead">Learn More</a>
                </li>
            </ul>
        </div>
        <!-- <div class="col-md-2 mx-0 align-items-center justify-content-center">
            <h5 class="font-weight-bold text-uppercase mt-5 mb-4">Get Involved</h5>
            <ul class="list-unstyled">
                <li>
                    <a href="current_members.html" class="lead">Current Members</a>
                </li>
                <li>
                    <a href="sponsors.html" class="lead">Support Us</a>
                </li>
            </ul>
        </div> -->

        <div class="col-md-3 mx-0 align-items-center justify-content-center" id="social-hosting">
            <div id="social-media">
                <a class="social-logo" href="https://www.facebook.com/fractalcs.org/" target="_"><img src="/imgs/facebook.svg"></a>
                <!-- <a class="social-logo" href="https://www.instagram.com/cs.fractal.ai/" target="_"><img src="/imgs/instagram.svg"></a> -->
                <!-- <a class="social-logo" href="https://medium.com/@cskickstart" target="_"><img src="imgs/medium.svg"></a> -->
                <!-- <a class="social-logo" href="https://twitter.com/kickstartcs" target="_"><img src="imgs/twitter.svg"></a> -->
            </div>
        </div>
    </section>
    <br><br>
</footer></body>

</html>
